{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sqMwlqbc7BqW"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import string as st\n",
        "import re\n",
        "import nltk\n",
        "from nltk import PorterStemmer, WordNetLemmatizer\n",
        "import nltk.corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfuhlkUHPorj"
      },
      "source": [
        "What Is NLTK ?\n",
        "\n",
        "The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP).\n",
        "\n",
        "It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning. It also includes graphical demonstrations and sample data sets as well as accompanied by a cook book and a book which explains the principles behind the underlying language processing tasks that NLTK supports.\n",
        "\n",
        "corpora and lexical sources NLTK includes more than 50 corpora and lexical sources such as the Penn Treebank Corpus, Open Multilingual Wordnet, Problem Report Corpus, and Linâ€™s Dependency Thesaurus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eiv-R8s58ghK",
        "outputId": "0fb9dd52-18bc-44f6-f45d-860bd54c1b93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\DELL\\Downloads\\ExperimetNo1.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DELL/Downloads/ExperimetNo1.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m nltk\u001b[39m.\u001b[39;49mdownload(\u001b[39m'\u001b[39;49m\u001b[39mall\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\nltk\\downloader.py:777\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow\u001b[39m(s, prefix2\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    769\u001b[0m     print_to(\n\u001b[0;32m    770\u001b[0m         textwrap\u001b[39m.\u001b[39mfill(\n\u001b[0;32m    771\u001b[0m             s,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    774\u001b[0m         )\n\u001b[0;32m    775\u001b[0m     )\n\u001b[1;32m--> 777\u001b[0m \u001b[39mfor\u001b[39;00m msg \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mincr_download(info_or_id, download_dir, force):\n\u001b[0;32m    778\u001b[0m     \u001b[39m# Error messages\u001b[39;00m\n\u001b[0;32m    779\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(msg, ErrorMessage):\n\u001b[0;32m    780\u001b[0m         show(msg\u001b[39m.\u001b[39mmessage)\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\nltk\\downloader.py:637\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(info, Collection):\n\u001b[0;32m    636\u001b[0m     \u001b[39myield\u001b[39;00m StartCollectionMessage(info)\n\u001b[1;32m--> 637\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mincr_download(info\u001b[39m.\u001b[39mchildren, download_dir, force)\n\u001b[0;32m    638\u001b[0m     \u001b[39myield\u001b[39;00m FinishCollectionMessage(info)\n\u001b[0;32m    640\u001b[0m \u001b[39m# Handle Packages (delegate to a helper function).\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\nltk\\downloader.py:624\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[39m# If they gave us a list of ids, then download each one.\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(info_or_id, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 624\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_list(info_or_id, download_dir, force)\n\u001b[0;32m    625\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[39m# Look up the requested collection or package.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\nltk\\downloader.py:667\u001b[0m, in \u001b[0;36mDownloader._download_list\u001b[1;34m(self, items, download_dir, force)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    666\u001b[0m     delta \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(item\u001b[39m.\u001b[39mpackages) \u001b[39m/\u001b[39m num_packages\n\u001b[1;32m--> 667\u001b[0m \u001b[39mfor\u001b[39;00m msg \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mincr_download(item, download_dir, force):\n\u001b[0;32m    668\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(msg, ProgressMessage):\n\u001b[0;32m    669\u001b[0m         \u001b[39myield\u001b[39;00m ProgressMessage(progress \u001b[39m+\u001b[39m msg\u001b[39m.\u001b[39mprogress \u001b[39m*\u001b[39m delta)\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\nltk\\downloader.py:642\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[39myield\u001b[39;00m FinishCollectionMessage(info)\n\u001b[0;32m    640\u001b[0m \u001b[39m# Handle Packages (delegate to a helper function).\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 642\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_package(info, download_dir, force)\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\nltk\\downloader.py:735\u001b[0m, in \u001b[0;36mDownloader._download_package\u001b[1;34m(self, info, download_dir, force)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[39mif\u001b[39;00m info\u001b[39m.\u001b[39munzip \u001b[39mor\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(zipdir, info\u001b[39m.\u001b[39mid)):\n\u001b[0;32m    734\u001b[0m     \u001b[39myield\u001b[39;00m StartUnzipMessage(info)\n\u001b[1;32m--> 735\u001b[0m     \u001b[39mfor\u001b[39;00m msg \u001b[39min\u001b[39;00m _unzip_iter(filepath, zipdir, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    736\u001b[0m         \u001b[39m# Somewhat of a hack, but we need a proper package reference\u001b[39;00m\n\u001b[0;32m    737\u001b[0m         msg\u001b[39m.\u001b[39mpackage \u001b[39m=\u001b[39m info\n\u001b[0;32m    738\u001b[0m         \u001b[39myield\u001b[39;00m msg\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\nltk\\downloader.py:2250\u001b[0m, in \u001b[0;36m_unzip_iter\u001b[1;34m(filename, root, verbose)\u001b[0m\n\u001b[0;32m   2247\u001b[0m     \u001b[39myield\u001b[39;00m ErrorMessage(filename, e)\n\u001b[0;32m   2248\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m-> 2250\u001b[0m zf\u001b[39m.\u001b[39;49mextractall(root)\n\u001b[0;32m   2252\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[0;32m   2253\u001b[0m     \u001b[39mprint\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\zipfile.py:1642\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[1;34m(self, path, members, pwd)\u001b[0m\n\u001b[0;32m   1639\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(path)\n\u001b[0;32m   1641\u001b[0m \u001b[39mfor\u001b[39;00m zipinfo \u001b[39min\u001b[39;00m members:\n\u001b[1;32m-> 1642\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(zipinfo, path, pwd)\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\zipfile.py:1697\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[1;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[39mreturn\u001b[39;00m targetpath\n\u001b[0;32m   1695\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(member, pwd\u001b[39m=\u001b[39mpwd) \u001b[39mas\u001b[39;00m source, \\\n\u001b[0;32m   1696\u001b[0m      \u001b[39mopen\u001b[39m(targetpath, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m target:\n\u001b[1;32m-> 1697\u001b[0m     shutil\u001b[39m.\u001b[39mcopyfileobj(source, target)\n\u001b[0;32m   1699\u001b[0m \u001b[39mreturn\u001b[39;00m targetpath\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJRqTfjM92NZ",
        "outputId": "92c94bcf-d093-4489-9053-42dc5fb53222"
      },
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mindian\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('indian')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/indian\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'c:\\\\Users\\\\DELL\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\DELL\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\DELL\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mindian\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('indian')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/indian.zip/indian/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'c:\\\\Users\\\\DELL\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\DELL\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\DELL\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\DELL\\Downloads\\ExperimetNo1.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL/Downloads/ExperimetNo1.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m indian\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DELL/Downloads/ExperimetNo1.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m py_text \u001b[39min\u001b[39;00m indian\u001b[39m.\u001b[39;49mfileids ():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL/Downloads/ExperimetNo1.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(py_text, indian\u001b[39m.\u001b[39mwords (py_text)[:\u001b[39m20\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mindian\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('indian')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/indian\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'c:\\\\Users\\\\DELL\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\DELL\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\DELL\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import indian\n",
        "for py_text in indian.fileids ():\n",
        "    print(py_text, indian.words (py_text)[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaNnUV_F-Oum"
      },
      "source": [
        "\"\"\"\n",
        "NLTK corpus readers.  The modules in this package provide functions\n",
        "that can be used to read corpus files in a variety of formats.  These\n",
        "functions can be used to read both the corpus files that are\n",
        "distributed in the NLTK corpus package, and corpus files that are part\n",
        "of external corpora.\" \" \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD3T7pBQBE_n",
        "outputId": "db45f6e8-0789-4ffc-92e7-be511d7c6464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a_and_c.xml', 'dream.xml', 'hamlet.xml', 'j_caesar.xml', 'macbeth.xml', 'merchant.xml', 'othello.xml', 'r_and_j.xml']\n",
            "a_and_c.xml ['The', 'Tragedy', 'of', 'Antony', 'and', 'Cleopatra', 'Dramatis', 'Personae', 'MARK', 'ANTONY', 'OCTAVIUS', 'CAESAR', 'M', '.', 'AEMILIUS', 'LEPIDUS', 'triumvirs', '.', 'SEXTUS', 'POMPEIUS']\n",
            "dream.xml ['A', 'Midsummer', 'Night', \"'\", 's', 'Dream', 'Dramatis', 'Personae', 'THESEUS', ',', 'Duke', 'of', 'Athens', '.', 'EGEUS', ',', 'father', 'to', 'Hermia', '.']\n",
            "hamlet.xml ['The', 'Tragedy', 'of', 'Hamlet', ',', 'Prince', 'of', 'Denmark', 'Dramatis', 'Personae', 'CLAUDIUS', ',', 'king', 'of', 'Denmark', '.', 'HAMLET', ',', 'son', 'to']\n",
            "j_caesar.xml ['The', 'Tragedy', 'of', 'Julius', 'Caesar', 'Dramatis', 'Personae', 'JULIUS', 'CAESAR', 'OCTAVIUS', 'CAESAR', 'MARCUS', 'ANTONIUS', 'M', '.', 'AEMILIUS', 'LEPIDUS', 'triumvirs', 'after', 'death']\n",
            "macbeth.xml ['The', 'Tragedy', 'of', 'Macbeth', 'Dramatis', 'Personae', 'DUNCAN', ',', 'king', 'of', 'Scotland', '.', 'MALCOLM', 'DONALBAIN', 'his', 'sons', '.', 'MACBETH', 'BANQUO', 'generals']\n",
            "merchant.xml ['The', 'Merchant', 'of', 'Venice', 'Dramatis', 'Personae', 'The', 'DUKE', 'OF', 'VENICE', '.', 'The', 'PRINCE', 'OF', 'MOROCCO', 'The', 'PRINCE', 'OF', 'ARRAGON', 'suitors']\n",
            "othello.xml ['The', 'Tragedy', 'of', 'Othello', ',', 'the', 'Moor', 'of', 'Venice', 'Dramatis', 'Personae', 'DUKE', 'OF', 'VENICE', 'BRABANTIO', ',', 'a', 'senator', '.', 'Other']\n",
            "r_and_j.xml ['The', 'Tragedy', 'of', 'Romeo', 'and', 'Juliet', 'Text', 'placed', 'in', 'the', 'public', 'domain', 'by', 'Moby', 'Lexical', 'Tools', ',', '1992', '.', 'XML']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import shakespeare\n",
        "print(shakespeare.fileids())\n",
        "for py_text in shakespeare.fileids ():\n",
        "    print(py_text, shakespeare.words (py_text)[:20])\n",
        "#w = shakespeare.words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM0zWrBlC_h7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n",
        "import nltk.corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz-jdR5qFSwv",
        "outputId": "5e9632e0-6281-404d-ce98-e566da601452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['kimmo.zip', 'words.zip', 'floresta.zip', 'knbc.zip', 'product_reviews_1.zip', 'nonbreaking_prefixes.zip', 'wordnet2021.zip', 'cess_esp', 'alpino.zip', 'brown.zip', 'bcp47.zip', 'brown_tei', 'conll2000.zip', 'timit', 'timit.zip', 'twitter_samples.zip', 'sinica_treebank', 'sentence_polarity', 'dolch.zip', 'sinica_treebank.zip', 'genesis', 'pe08.zip', 'nonbreaking_prefixes', 'product_reviews_1', 'wordnet.zip', 'udhr', 'webtext.zip', 'floresta', 'subjectivity.zip', 'gutenberg.zip', 'senseval.zip', 'cmudict.zip', 'treebank.zip', 'movie_reviews.zip', 'product_reviews_2.zip', 'shakespeare.zip', 'swadesh', 'conll2002', 'kimmo', 'verbnet', 'webtext', 'sentiwordnet', 'indian', 'europarl_raw.zip', 'ieer.zip', 'chat80.zip', 'toolbox', 'opinion_lexicon.zip', 'names', 'toolbox.zip', 'paradigms.zip', 'switchboard.zip', 'senseval', 'comtrans.zip', 'subjectivity', 'mte_teip5.zip', 'stopwords', 'nombank.1.0.zip', 'smultron', 'state_union', 'treebank', 'words', 'lin_thesaurus', 'wordnet2022', 'machado.zip', 'wordnet31.zip', 'gutenberg', 'cess_cat', 'wordnet_ic.zip', 'masc_tagged.zip', 'verbnet3', 'wordnet2022.zip', 'pl196x', 'ycoe.zip', 'cess_cat.zip', 'city_database', 'twitter_samples', 'brown', 'qc', 'ptb.zip', 'conll2002.zip', 'framenet_v15.zip', 'udhr2.zip', 'omw-1.4.zip', 'framenet_v15', 'crubadan', 'names.zip', 'crubadan.zip', 'comparative_sentences', 'mac_morpho', 'verbnet3.zip', 'comparative_sentences.zip', 'biocreative_ppi', 'qc.zip', 'ieer', 'ptb', 'panlex_swadesh.zip', 'unicode_samples', 'rte.zip', 'pros_cons.zip', 'switchboard', 'reuters.zip', 'cess_esp.zip', 'dependency_treebank', 'mte_teip5', 'opinion_lexicon', 'conll2000', 'udhr.zip', 'gazetteers', 'udhr2', 'ppattach.zip', 'mac_morpho.zip', 'omw.zip', 'brown_tei.zip', 'verbnet.zip', 'state_union.zip', 'movie_reviews', 'europarl_raw', 'conll2007.zip', 'propbank.zip', 'product_reviews_2', 'ycoe', 'city_database.zip', 'abc.zip', 'pil', 'rte', 'genesis.zip', 'inaugural', 'dolch', 'problem_reports.zip', 'abc', 'cmudict', 'extended_omw.zip', 'shakespeare', 'chat80', 'pe08', 'inaugural.zip', 'gazetteers.zip', 'biocreative_ppi.zip', 'paradigms', 'lin_thesaurus.zip', 'ppattach', 'alpino', 'framenet_v17', 'problem_reports', 'framenet_v17.zip', 'nps_chat', 'sentiwordnet.zip', 'nps_chat.zip', 'universal_treebanks_v20.zip', 'pil.zip', 'jeita.zip', 'swadesh.zip', 'stopwords.zip', 'smultron.zip', 'wordnet_ic', 'dependency_treebank.zip', 'pl196x.zip', 'pros_cons', 'unicode_samples.zip', 'indian.zip', 'semcor.zip', 'sentence_polarity.zip']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir(nltk.data.find(\"corpora\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVqUcQt4Fgsj",
        "outputId": "456bf204-787c-4cf6-c5d5-6cf1721818bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import brown\n",
        "brown.words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IrnbZX0Fufn",
        "outputId": "a357d1b4-5565-4d2b-aace-65e05195591d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK7YLYFcGpxe",
        "outputId": "64481b54-a9c3-4c73-bc1b-4ef8dde571f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hamlet=nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
        "hamlet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTqpJyN0HIiy",
        "outputId": "7d38b918-ea5a-4350-e834-177f514b149f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo ? Bar . He Fran . You come most carefully vpon your houre Bar . ' Tis now strook twelue , get thee to bed Francisco Fran . For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart Barn . Haue you had quiet Guard ? Fran . Not a Mouse stirring Barn . Well , goodnight . If you do meet Horatio and Marcellus , the Riuals of my Watch , bid them make hast . Enter Horatio and Marcellus . Fran . I thinke I heare them . Stand : who ' s there ? Hor . Friends to this ground Mar . And Leige - men to the Dane Fran . Giue you good night Mar . O farwel honest Soldier , who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s Westward from the Pole Had made his course t ' illume that part of Heauen Where now it burnes , Marcellus and my selfe , The Bell then beating one Mar . Peace , breake thee of : Enter the Ghost . Looke where it comes againe Barn . In the same figure , like the King that ' s dead Mar . Thou art a Scholler ; speake to it Horatio Barn . Lookes it not like the King ? Marke it Horatio Hora . Most like : It harrowes me with fear & wonder Barn . It would be spoke too Mar . Question it Horatio Hor . What art "
          ]
        }
      ],
      "source": [
        "for words in hamlet[:500]:\n",
        "  print(words,sep=' ',end=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF_20vZNywzk",
        "outputId": "dea59eee-9af4-4e9e-fd30-647c93d686b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]\n",
            "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "hamlet_str=str(hamlet)\n",
        "hamlet_str2=str(hamlet)\n",
        "print(hamlet_str)\n",
        "print(hamlet_str2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3DV1PbM0zR0",
        "outputId": "95c1faff-9b14-40b6-a25c-f1be4da5eba3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"['[',\", \"'The',\", \"'Tragedie',\", \"'of',\", \"'Hamlet',\", \"'by',\", '...]']"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Simple tokenization with .split simplest method to perform tokenization in Python. If you type .split(), the text will be separated at each blank space.\n",
        "#hamlet_new=hamlet_str\n",
        "#hamlet_new.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLdOAMvo2ERc",
        "outputId": "4675e997-05c2-4b98-c014-ab9bea5079b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]\"]\n",
            "['[', \"'\", '[', \"'\", ',', \"'The\", \"'\", ',', \"'Tragedie\", \"'\", ',', \"'of\", \"'\", ',', \"'Hamlet\", \"'\", ',', \"'by\", \"'\", ',', '...', ']']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "word_tokenize(hamlet_str2)\n",
        "print(sent_tokenize(hamlet_str2))\n",
        "print(word_tokenize(hamlet_str2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gF8iXZgBJnPT",
        "outputId": "68b541b0-12cd-44f0-d649-99de5d8c9c63"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"['[', 'the', 'tragedie', 'of', 'hamlet', 'by', ...]\""
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Text Lower Case\n",
        "def text_lowercase(text):\n",
        "  return text.lower()\n",
        "\n",
        "text_lowercase(hamlet_str2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF9s4inzJ5Qu",
        "outputId": "1ca5c4fe-d391-4bcb-91b7-e1837b889fac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are  balls in this bag, and  in the other one.\n"
          ]
        }
      ],
      "source": [
        "#Remove Numbers\n",
        "\n",
        "def remove_numbers(text):\n",
        "  result = re.sub(r'\\d+', '', text)\n",
        "  return result\n",
        "input_str = \"There are 3 balls in this bag, and 12 in the other one.\"\n",
        "print(remove_numbers(input_str) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8k4vaUFKPCm",
        "outputId": "caa42ae7-a9ac-4d9c-a711-53d840ba3437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hey did you know that the summer break is coming Amazing right  Its only 5 more days \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "def remove_punctuation(text):\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  return text.translate(translator)\n",
        "input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\"\n",
        "print(remove_punctuation(input_str) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbRh8FQZKgbt",
        "outputId": "1b6f2b5e-4368-4f51-9a5e-c1b57e7cbaa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "we don't need the given questions\n"
          ]
        }
      ],
      "source": [
        "#Remove Whitspaces\n",
        "import nltk\n",
        "def remove_whitespace(text):\n",
        "  return \" \".join(text.split())\n",
        "input_str = \" we don't need   the given       questions\"\n",
        "print(remove_whitespace(input_str) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrnkzNGaK4_Y",
        "outputId": "bf3a6bf9-855b-466f-bfac-61cbe2cd0d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['need', 'given', 'questions']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# remove stopwords function\n",
        "def remove_stopwords(text):\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "  return filtered_text\n",
        "text=\"we do not need the given questions\"\n",
        "print(remove_stopwords(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwNcaWEBO9r5",
        "outputId": "fb3af19a-b0ae-4407-cfc1-a282e4f625bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['we', 'do', 'not', 'need', 'the', 'given', 'questions']\n",
            "<FreqDist with 3 samples and 3 outcomes>\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "tokens = [t for t in text.split()] \n",
        "print(tokens)\n",
        "sr= stopwords.words('english')\n",
        "clean_tokens = tokens[:]\n",
        "for token in tokens:\n",
        "  if token in stopwords.words('english'):\n",
        "    clean_tokens.remove(token)\n",
        "    freq = nltk.FreqDist(clean_tokens)\n",
        "print(freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEA7R4n9PC0X"
      },
      "outputs": [],
      "source": [
        "#Stemming:porter Stemmer \n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
        "... is no basis for a system of government. Supreme executive power derives from\n",
        "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
        "tokens = word_tokenize(raw)\n",
        "porter = nltk.PorterStemmer() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKJ_8LtoPORs",
        "outputId": "ee5451a5-6878-4554-fbce-4d9db237980f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['data', 'science', 'use', 'scientific', 'methods', 'algorithms', 'and', 'many', 'type', 'of', 'process']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# lemmatize string\n",
        "def lemmatize_word(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  # provide context i.e. part-of-speech\n",
        "  lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
        "  return lemmas\n",
        "text = 'data science uses scientific methods algorithms and many types of processes'\n",
        "print(lemmatize_word(text) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbaBk60RPl0S"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Qp-thhPvJmbx",
        "outputId": "600884f8-73a0-49d3-909f-d8afd066f3d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8f01f417-0693-451c-9658-8d27b96fb22d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RT @rssurjewala: Critical question: Was PayTM ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RT @Hemant_80: Did you vote on #Demonetization...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RT @roshankar: Former FinSec, RBI Dy Governor,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @ANI_news: Gurugram (Haryana): Post office ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @satishacharya: Reddy Wedding! @mail_today ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f01f417-0693-451c-9658-8d27b96fb22d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8f01f417-0693-451c-9658-8d27b96fb22d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8f01f417-0693-451c-9658-8d27b96fb22d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text\n",
              "0  RT @rssurjewala: Critical question: Was PayTM ...\n",
              "1  RT @Hemant_80: Did you vote on #Demonetization...\n",
              "2  RT @roshankar: Former FinSec, RBI Dy Governor,...\n",
              "3  RT @ANI_news: Gurugram (Haryana): Post office ...\n",
              "4  RT @satishacharya: Reddy Wedding! @mail_today ..."
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "df=pd.read_csv('/content/tweets.csv',encoding='cp1252')\n",
        "df_text=df[['text']]\n",
        "df_text.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
